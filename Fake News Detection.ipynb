{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11282918,"sourceType":"datasetVersion","datasetId":7054304}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport proselint\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nnltk.download('averaged_perceptron_tagger_eng')\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nfrom nltk.tokenize import word_tokenize\nfrom nltk import pos_tag\n\n\nclass ConvolutionalNetwork(nn.Module):\n    def __init__(self):\n        super(ConvolutionalNetwork, self).__init__()\n\n        # More dense 1D Convolutional layers\n        self.conv_layer = nn.Sequential(\n            nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1),  # (64, 17)\n            nn.Tanh(),\n            nn.MaxPool1d(kernel_size=2),  # (64, 8)\n\n            nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1),  # (128, 8)\n            nn.Tanh(),\n            nn.MaxPool1d(kernel_size=2),  # (128, 4)\n\n            nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1),  # (256, 4)\n            nn.Tanh(),\n            nn.MaxPool1d(kernel_size=2),  # (256, 2)\n\n            nn.Conv1d(256, 512, kernel_size=3, stride=1, padding=1),  # (512, 2)\n            nn.Tanh(),\n            nn.MaxPool1d(kernel_size=2),  # (512, 1)\n\n            nn.Conv1d(512, 1024, kernel_size=3, stride=1, padding=1),  # (1024, 1)\n            nn.Tanh(),\n        )\n\n        # Fully connected layers after the convolutional layers\n        self.fc_layer = nn.Sequential(\n            nn.Linear(1024, 512),  \n            nn.Tanh(),\n            nn.Linear(512, 128),\n            nn.Tanh(),\n            nn.Linear(128, 2), \n        )\n\n    def forward(self, x):\n        x = x.view(x.size(0), 1, 17) \n        x = self.conv_layer(x) \n        x = torch.flatten(x, 1)  \n        x = self.fc_layer(x)  \n        return x\n\n\n# Load the model\n\ndef preprocess(title, text, subject, date):\n    # Step 1 : Checking punctuation\n    def check_punctuation(headline):\n        issues = proselint.tools.lint(headline)\n        # print(\"checking\")\n        if(not len(issues)):\n            return 1\n        else:\n            return 0\n    \n    # Step 2 : title length, text length, text sentence length\n    stop_words = set(stopwords.words('english'))  # Get English stop words\n\n    def title_length(title):\n        # remove stop words and count the length of the title\n        tit_len = len(title)\n        title = title.split()\n        title = [word for word in title if word.lower() not in stop_words]\n        # count number of chars in title without stop words\n        return [tit_len, len(title)]\n\n\n    def text_length(text):\n        # remove stop words and count the length of the text\n        tex_len = len(text)\n        text = text.split()\n        text = [word for word in text if word.lower() not in stop_words]\n        return [tex_len, len(text)]\n    \n    def text_sentence_count(text):\n        # count number of sentences in the text\n        # return text.fillna(\"\").apply(lambda x: len(nltk.sent_tokenize(x)))\n        return len(nltk.sent_tokenize(text))\n    \n    # Step 3 : Sentiment Analysis\n    def sentiment_analyzer_scores(text):\n        analyser = SentimentIntensityAnalyzer()\n        score = analyser.polarity_scores(text)\n        return [score['neg'], score['neu'], score['pos'], score['compound']]\n    \n    # Step 4 : Cateory ID\n    def category_id(subject):\n        categories = ['politicsNews', 'Government News', 'left-news', 'politics', 'worldnews', 'News', 'Middle-east', 'US_News']\n        return categories.index(subject) + 1\n    \n    # Step 5 : Keyword Density\n    def title_keyword_density(title):\n        headline = title.strip().lower()\n        tokens = word_tokenize(headline)\n        tags = pos_tag(tokens)\n\n        jj_count = sum(1 for word, tag in tags if tag == 'JJ')\n        vbg_count = sum(1 for word, tag in tags if tag == 'VBG')\n        rb_count = sum(1 for word, tag in tags if tag == 'RB')\n        total_count = len(tokens)\n\n        jj_density = (jj_count / total_count) * 100 if total_count > 0 else 0\n        vbg_density = (vbg_count / total_count) * 100 if total_count > 0 else 0\n        rb_density = (rb_count / total_count) * 100 if total_count > 0 else 0\n\n        return [jj_density, vbg_density, rb_density]\n\n    def text_keyword_density(text):\n        headline = text.strip().lower()\n        tokens = word_tokenize(headline)\n        tags = pos_tag(tokens)\n\n        jj_count = sum(1 for word, tag in tags if tag == 'JJ')\n        vbg_count = sum(1 for word, tag in tags if tag == 'VBG')\n        rb_count = sum(1 for word, tag in tags if tag == 'RB')\n        total_count = len(tokens)\n\n        jj_density = (jj_count / total_count) * 100 if total_count > 0 else 0\n        vbg_density = (vbg_count / total_count) * 100 if total_count > 0 else 0\n        rb_density = (rb_count / total_count) * 100 if total_count > 0 else 0\n\n        return [jj_density, vbg_density, rb_density]\n    \n    return [category_id(subject), *title_length(title), *text_length(text), text_sentence_count(text), *sentiment_analyzer_scores(text), check_punctuation(title), *title_keyword_density(title), *text_keyword_density(text)]\n\nmodel = torch.load('/kaggle/input/fakeyy/model_complete.pth', weights_only=False)\n\nmodel.eval()\n\n# Read CSV\ndf = pd.read_csv(\"/kaggle/input/truedata/True.csv\")\n\nfake_count = 0\ntotal = 0\n\nfor index, row in df.iterrows():\n    title = row['title']\n    text = row['text']\n    subject = row['subject']\n    date = row['date']\n\n    try:\n        # Preprocess\n        test_input = preprocess(title, text, subject, date)\n        test_input = torch.tensor(test_input).float().unsqueeze(0)\n\n        # Inference\n        with torch.no_grad():\n            outputs = model(test_input)\n            probabilities = F.softmax(outputs, dim=1)\n            prediction = torch.argmax(probabilities, dim=1)\n\n        if prediction.item() == 1:\n            fake_count += 1\n        total += 1\n    except Exception as e:\n        print(f\"Error processing row {index}: {e}\")\n\n    if(index % 100 == 0):\n        print(\"done error \", fake_count / total)\n\nprint(\"\\n=============================================\")\nprint(f\"Total Articles: {total}\")\nprint(f\"Fake Predictions: {fake_count}\")\nprint(\"=============================================\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-06T12:49:22.727804Z","iopub.execute_input":"2025-04-06T12:49:22.728162Z","iopub.status.idle":"2025-04-06T14:47:54.285110Z","shell.execute_reply.started":"2025-04-06T12:49:22.728132Z","shell.execute_reply":"2025-04-06T14:47:54.284107Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  1.0\ndone error  0.9998387356877922\ndone error  0.9998412950325345\ndone error  0.9998437744102484\ndone error  0.9998461775111521\ndone error  0.9998485078018482\ndone error  0.9998507685420086\ndone error  0.9998529627995882\ndone error  0.9998550934647152\ndone error  0.999857163262391\ndone error  0.9998591747641177\ndone error  0.9998611303985557\ndone error  0.9998630324613067\ndone error  0.9998648831239022\ndone error  0.9998666844420744\ndone error  0.9998684383633732\ndone error  0.9998701467341904\ndone error  0.9998718113062428\ndone error  0.9998734337425642\ndone error  0.9998750156230471\ndone error  0.9998765584495741\ndone error  0.9998780636507743\ndone error  0.9998795325864354\ndone error  0.999880966551601\ndone error  0.9998823667803788\ndone error  0.9998837344494826\ndone error  0.9998850706815309\ndone error  0.9998863765481195\ndone error  0.9998876530726885\ndone error  0.9998889012331963\ndone error  0.9998901219646192\ndone error  0.9998913161612868\ndone error  0.9998924846790668\ndone error  0.9998936283374109\ndone error  0.9998947479212714\ndone error  0.9998958441828976\ndone error  0.9998969178435213\ndone error  0.9998979695949393\ndone error  0.9998990001009999\ndone error  0.9999000099990001\ndone error  0.9999009999009999\ndone error  0.9999019703950593\ndone error  0.9999029220464033\ndone error  0.9999038553985193\ndone error  0.999904770974193\ndone error  0.9999056692764834\ndone error  0.9999065507896459\ndone error  0.9999074159800019\ndone error  0.9999082652967618\ndone error  0.9999090991728025\ndone error  0.9999099180254031\ndone error  0.9999107222569413\ndone error  0.9999115122555526\ndone error  0.9999122883957547\ndone error  0.9998261020780802\ndone error  0.9998276010688734\ndone error  0.9997436116571233\ndone error  0.9997457842555716\ndone error  0.9996638937904377\ndone error  0.9995833680526622\ndone error  0.9995868110073548\ndone error  0.9995901975247931\ndone error  0.9995935289813836\ndone error  0.9995968067091363\ndone error  0.9995200383969283\ndone error  0.9994444885326561\ndone error  0.9994488622943075\ndone error  0.9994531677212718\ndone error  0.9994574064026045\ndone error  0.9994615798784708\ndone error  0.9993130295397298\ndone error  0.9992424816301795\ndone error  0.9992481768288098\ndone error  0.9992537870308186\ndone error  0.9992593141248797\ndone error  0.9992647599441218\ndone error  0.9991971388949712\ndone error  0.999202956307514\ndone error  0.9992086900223005\ndone error  0.9992143418327263\ndone error  0.9992199134813133\ndone error  0.9992254066615027\ndone error  0.999160897839312\ndone error  0.9991667245330186\ndone error  0.9991724708640783\ndone error  0.999109650023971\ndone error  0.9991157064145296\ndone error  0.9990541179650023\ndone error  0.9990604657405543\ndone error  0.999066728884741\ndone error  0.9990729090788689\ndone error  0.9990132228142885\ndone error  0.9990196719168681\ndone error  0.9990260372703071\ndone error  0.9990323204954519\ndone error  0.9990385231715916\ndone error  0.9989809566269664\ndone error  0.9989874058603886\ndone error  0.9989937739764795\ndone error  0.999000062496094\ndone error  0.9989441649586982\ndone error  0.998888957471761\ndone error  0.9988957732654439\ndone error  0.9989025059447595\ndone error  0.9989091570207866\ndone error  0.9989157279681947\ndone error  0.9988623435722411\ndone error  0.998869114933635\ndone error  0.9988758061653157\ndone error  0.998882418681254\ndone error  0.9988889538623472\ndone error  0.9988954130573804\ndone error  0.9987861973296341\ndone error  0.9987931728061605\ndone error  0.9988000685675105\ndone error  0.998806885972388\ndone error  0.9988136263487939\ndone error  0.9987641143755969\ndone error  0.9986592927769399\ndone error  0.9986667407366258\ndone error  0.9986188608364179\ndone error  0.9985715070600516\ndone error  0.9985246707830173\ndone error  0.9984783435682844\ndone error  0.9984865682936057\ndone error  0.9984947045857749\ndone error  0.9985027538634298\ndone error  0.9985107175150258\ndone error  0.998518596899635\ndone error  0.9985263933477185\ndone error  0.9985341081618764\ndone error  0.998489661996771\ndone error  0.9984974871768302\ndone error  0.9985052316890882\ndone error  0.9985128967745244\ndone error  0.9985204836487934\ndone error  0.9985279935028679\ndone error  0.9985354275036614\ndone error  0.9985427867946335\ndone error  0.9985500724963752\ndone error  0.9985572857071787\ndone error  0.9985149250037126\ndone error  0.9984729816265209\ndone error  0.998480466643792\ndone error  0.9984878786400664\ndone error  0.9984952186787049\ndone error  0.9985024878025216\ndone error  0.9985096870342772\ndone error  0.998516817377159\ndone error  0.9985238798152469\ndone error  0.9985308753139661\ndone error  0.9985378048205273\ndone error  0.998497723111591\ndone error  0.9985047427690295\n\n=============================================\nTotal Articles: 21417\nFake Predictions: 21385\n=============================================\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"print(\"accuracy is : \", fake_count / total * 100)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T14:48:05.579913Z","iopub.execute_input":"2025-04-06T14:48:05.580315Z","iopub.status.idle":"2025-04-06T14:48:05.584914Z","shell.execute_reply.started":"2025-04-06T14:48:05.580278Z","shell.execute_reply":"2025-04-06T14:48:05.584155Z"}},"outputs":[{"name":"stdout","text":"accuracy is :  99.85058598309755\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}